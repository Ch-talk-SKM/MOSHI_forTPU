

##  í•µì‹¬ ëª¨ë“ˆë³„ â€œêµ¬í˜„ ìƒíƒœâ€ ì²´í¬ë¦¬ìŠ¤íŠ¸ (ì´ 18 ì—”í‹°í‹°)

| #  | ëª¨ë“ˆ (Upstream ì´ë¦„)                  | Upstream í´ë”               | **ê¸°ëŠ¥**                         | **TPU í¬íŒ… í˜„í™©**                   | ë¯¸êµ¬í˜„Â·ê²°í•¨                        |
| -- | --------------------------------- | ------------------------- | ------------------------------ | ------------------------------- | ----------------------------- |
| 1  | **FlexibleLinear**                | `moshi/modeling_torch.py` | ì±„ë„ë³„ ì½”ë“œë¶-ì„ íƒ ì„ í˜•ì¸µ                 | PT+Flax êµ¬í˜„, unit-test í†µê³¼        | `jax.precision` ì˜µì…˜ ì—†ìŒ         |
| 2  | **RMSNorm**                       | same                      | ìŠ¤ì¼€ì¼ë…¸ë¦„                          | PT+Flax êµ¬í˜„                      | dtype cast/eps config ë¶ˆì™„ì „     |
| 3  | **GatedMLP**                      | same                      | GEGLU feed-forward             | êµ¬í˜„ ì™„ë£Œ                           | noâ€gate fallback ì—†ìŒ           |
| 4  | **RotaryEmbedding**               | same                      | RoPE Î¸ ìŠ¤ì¼€ì¼Â·ë™ì  cache            | êµ¬í˜„ ì™„ë£Œ                           | Flax ìª½ `precision=HIGHEST` ëˆ„ë½ |
| 5  | **MultiHeadAttention**            | same                      | SDPA with key/value cache      | PT+Flax ë²„ì „ ìˆìœ¼ë‚˜ **Flash-Attn X** |                               |
| 6  | **DecoderLayer**                  | same                      | (Attn+MLP+Norm) block          | PT+Flax unit-test í†µê³¼            | â€”                             |
| 7  | **DepthDecoder**                  | same                      | codebook ê°„ ì¢…ë‹¨ê°„ ì˜ì¡´ ëª¨ë¸           | PT+Flax test OK                 | ì‹¤ì œ í•™ìŠµÂ·loss curve ê²€ì¦ ë¯¸ì™„        |
| 8  | **TemporalTransformer 7B**        | same                      | 1 k layer GPT-style backbone   | **ë¯¸êµ¬í˜„**                         | 7 B param init & ckpt load    |
| 9  | **InnerMonologueTextHead**        | same                      | í…ìŠ¤íŠ¸ í† í° ë³´ì¡° LM head              | ì—†ìŒ                              | Loss weight ìŠ¤ì¼€ì¤„ í¬í•¨í•´ì•¼          |
| 10 | **MoshiForConditionalGeneration** | same                      | HF `generate()` wrapper        | ì—†ìŒ                              | Beam search, logits mask      |
| 11 | **MimiCodec (Rust)**              | `rust/`, `audio/`         | 24 kHz stream codec â†’ tokens   | ì „ë¬´                              | PyO3 wheel â†’ XLA FFI          |
| 12 | **prepare\_dataset.py**           | `scripts/`                | wavâ†’npz ë°°ì¹˜ í† í¬ë‚˜ì´ì €               | ì—†ìŒ                              | Shard index, bucketing        |
| 13 | **train.py**                      | `scripts/`                | DeepSpeed, Flash-Attn8, ZeRO-3 | Flax ë‹¨ì¼-core ì˜ˆì‹œë§Œ                | pjit mesh / grad accum        |
| 14 | **inference\_server.py**          | `client/`                 | WebSocket duplex, VAD          | ì—†ìŒ                              | Opus/PCM stream, beam sync    |
| 15 | **MLX variant**                   | `moshi_mlx/`              | iOS Metal backend              | ì—†ìŒ                              | ì„ íƒ ì‚¬í•­                         |
| 16 | **CI workflow**                   | `.github/workflows`       | ruff, pytest, rust-fmt         | ì—†ìŒ                              | CPU jaxlib, torch compile     |
| 17 | **docs site**                     | docs GitHub Pages         | API + paper figures            | ì—†ìŒ                              | Sphinx-autodoc                |
| 18 | **Docker compose**                | root                      | GPU demo stack                 | ì—†ìŒ                              | TPU VM Dockerfile ì‘ì„±          |

---

### 3  ì„¸ë¶€ ê°­ í•´ì„¤ (ì˜ˆì‹œ)

* **Codec â†” Model íƒ€ì´ë° ë§ì¶¤**
  MimiëŠ” **80 ms í”„ë ˆì„(12.5 Hz)** ì¶œë ¥ì´ë¯€ë¡œ, DepthDecoder 1 step == 1 frameì„ì„ ë³´ì¥í•´ì•¼ í•¨. TPU í¬íŒ…ë¶„ì—” ì½”ë±ì´ ì—†ì–´ ì‹œê°„ì¶• alignment ê²€ì¦ ìì²´ê°€ ë¶ˆê°€ëŠ¥í•˜ë‹¤.
* **Flash-Attention in XLA**
  Upstreamì€ `FlashAttention2` CUDAë¥¼ `torchscale` forkë¡œ í˜¸ì¶œ; TPUì—ì„œëŠ” `jax.lax.dot_general` rewire or `xla.call_tf_kernel("flash_attention", â€¦)` ë¡œ ëŒ€ì²´ êµ¬í˜„ í•„ìš”.
* **Distributed pjit Mesh**
  ìµœì  ìƒ¤ë”©: `('dp','fsdp')` â†’ (`dp` = host-level data-parallel, `fsdp` = layer-wise param shards). `ShardingSpec` ë¯¸ì„¤ì • ì‹œ XLA ë©”ëª¨ë¦¬ í„°ì§€ë¯€ë¡œ í•„ìˆ˜.


---


## 1. `layer_unit_test_TPU.py` 

| ë²”ì£¼             | ì„¸ë¶€ ë‚´ìš©                                                                                                                                                                                        |
| -------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ì£¼ìš” ì •ì˜**      | `MoshiFlexibleLinearPT/FL`, `MoshiRMSNormPT/FL`, `MoshiGatingMLPPT/FL` â€• **PT â†” Flax 1 : 1 ëŒ€ì‘**                                                                                              |
| **ì…ë ¥ â†’ ì¶œë ¥ ê·œì¹™** | `layer_idx` ì¸ìì˜ 3-way ëª¨ë“œ<br>â‘  `None` â†’ í† í°ì¶• S ê°€ ê³§ codebook ì¶• (*S == num\_layers*)<br>â‘¡ `int` â†’ ëª¨ë“  í† í°ì´ ë™ì¼ ì½”ë“œë¶ ì‚¬ìš©<br>â‘¢ `1-D tensor/array` â†’ token-wise codebook ì„ íƒ                              |
| **ê²€ì¦ ë¡œì§**      | (ì•„ë˜ ë‘ í•¨ìˆ˜ê°€ íŒŒì¼ ëì— ì¡´ì¬í•˜ì§€ë§Œ raw ë³¸ë¬¸ì´ 1 ì¤„ë¡œ ë°˜í™˜ë¼ ìœ„ì¹˜ ì¸ë±ì‹±ì€ ë¶ˆê°€) <br>â€£ `copy_pt_weights_to_flax()` : state-dict â†’ FrozenDict ë³€í™˜ <br>â€£ `run_single_case()` : ë‚œìˆ˜ ì…ë ¥ ìƒì„± â†’ ë‘ í”„ë ˆì„ì›Œí¬ forward â†’ MAE â‰¤ 1e-6 assert |
| **ë””ìì¸ í¬ì¸íŠ¸**    | *FlexibleLinear* ì—ì„œ (B,S,D) â†” (B,1,D) ìë™ ë¸Œë¡œë“œìºìŠ¤íŠ¸ / `matmul_2d` einsum == `torch.nn.functional.linear` ëŒ€ì¹­                                                                                      |
| **ë°œê²¬ëœ ë¬¸ì œ**     | â‘  íŒŒì¼ì— í´ë˜ìŠ¤ + í…ŒìŠ¤íŠ¸ ì½”ë“œê°€ í•¨ê»˜ ë“¤ì–´ ìˆì–´ **ì¬ì‚¬ìš© ì–´ë ¤ì›€**<br>â‘¡ í•¨ìˆ˜Â·í´ë˜ìŠ¤ Docstring ì€ ìˆì§€ë§Œ **type hint ì—†ìŒ**<br>â‘¢ Flax ë²„ì „ì˜ ê²½ìš° XLAâ€friendly einsum ì´ì§€ë§Œ `precision=lax.Precision.HIGHEST` ê°™ì€ ì˜µì…˜ ë¯¸ì§€ì •                   |

---

## 2. `layer_unit_test_TPU_compare3.py` 

| ë²”ì£¼          | ì„¸ë¶€ ë‚´ìš©                                                                                                                                                          |
| ----------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ëª©ì **      | **â€œì›ë³¸ PyTorchâ€(`*_PT_origin`) vs ìƒˆ PyTorch(`*_PT`) vs Flax(`*_FL`)** â€“â€“â€“ ì„¸ ë³€ì¢…ì„ í•œêº¼ë²ˆì— ë¹„êµ                                                                         |
| **í•µì‹¬ ìœ í‹¸**   | `compare_two_model_outputs()` (PT â†” PT) Â· `compare_pt_fl()` (PT â†” FL) â€• ë‘˜ ë‹¤ MAE ì¶œë ¥                                                                             |
| **ì¤‘ìš” êµ¬í˜„**   | *PT\_origin* í´ë˜ìŠ¤ë“¤ì€ **HF `modeling_moshi.py` ì›í˜•** ì¼ë¶€ë¥¼ ê·¸ëŒ€ë¡œ ì˜®ê¸´ ê²ƒ. Flexible-Linear ê°€ `forward` ì—ì„œ **int Â· 1-D Â· None** ì¼€ì´ìŠ¤ë¥¼ ëª¨ë‘ ì§€ì›í•˜ë˜ *shape ë¶ˆì¼ì¹˜ ì‹œ ì—ëŸ¬* ë¥¼ ëª…ì‹œì ìœ¼ë¡œ raise |
| **íŠ¹ì´ì **     | ë™ì¼ íŒŒì¼ ì•ˆì— **ë™í˜• í´ë˜ìŠ¤ê°€ 6 ê°œ**(PT\_origin/PT/FL Ã— 3) â†’ DRY ìœ„ë°°, ì°¨í›„ íŒ¨í‚¤ì§€í™” ì‹œ í†µíí•© í•„ìš”                                                                                      |
| **ë¡œì§ ì•ˆì „ì¥ì¹˜** | MAE ê³„ì‚°ì´ `np.abs(diff).mean()` í•˜ë‚˜ë¿ â†’ Max diff ê²€ì¦ì´ë‚˜ dtype-cast ì˜¤ë¥˜ íƒì§€ëŠ” ì—†ìŒ                                                                                         |

---

## 3. `layer_unit_test_TPU_att_rope.py` 

| ë²”ì£¼              | ì„¸ë¶€ ë‚´ìš©                                                                                                                                                           |
| --------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ì¶”ê°€ ë ˆì´ì–´**      | `MoshiRotaryEmbeddingPT`, `MoshiLinearPT`(í‘œì¤€ vs Flexible wrapper), `apply_rotary_pos_emb` í—¬í¼                                                                    |
| **RoPE ì´ˆê¸°í™” ì „ëµ** | ë”•ì…”ë„ˆë¦¬ `ROPE_INIT_FUNCTIONS`<br> â€¢ `default` : Î˜ = 10 000 <br> â€¢ `dynamic` : Î˜ = `config.rope_theta`, scaling âˆ `seq_len^-0.05`                                   |
| **ì£¼ì˜ ê¹Šê²Œ ë³¸ ë¶€ë¶„**  | `MoshiRotaryEmbeddingPT._dynamic_frequency_update()` : **position IDsê°€ ê¸°ì¡´ cache ê¸¸ì´ ì´ˆê³¼** ì‹œ inv-freq ì¬ê³„ì‚° â†’ **ì¤„ì–´ë“  ê¸¸ì´**(fine-tune short seq) ì—ì„œëŠ” *ì´ˆê¸° inv-freq ë¡œ ë¡¤ë°±* |
| **ê²°í•© í…ŒìŠ¤íŠ¸**      | ì•„ì§ Attention ìì²´ëŠ” í¬í•¨ ì•ˆ ë¨ â†’ RoPE ê°’ë§Œ ê²€ì¦(ì½”ì‚¬ì¸Â·ì‚¬ì¸ í…Œì´ë¸”)                                                                                                                |

---

## 4. `layer_unit_test_TPU_att_rope_compare3.py` 

> â€œ3-way compareâ€ ë²„ì „ì˜ **Attention + RoPE**. PT\_origin â†” PT â†” FL ëª¨ë‘ ìƒì„±í•˜ê³ , `use_flexible_linear` í”Œë˜ê·¸ì— ë”°ë¼ ë™ì¼ weight copy êµ¬ê°„ì´ ë‹¬ë¼ì§„ë‹¤.
> *Flax ìª½ Attention* ì€ `MoshiAttentionFL` ë¡œ ì´ë¯¸ í¬í•¨ë¼ ìˆìœ¼ë‚˜, íŒŒì¼ ì•ë¶€ë¶„ ì£¼ì„ì—ë§Œ ëª…ì‹œë  ë¿ raw ë³¸ë¬¸ì€ ê¸¸ì–´ì„œ ì—¬ê¸° ì¸ìš©ì—” ë…¸ì¶œ X.

**ê´€ì°°**

* ì„¸ ë²„ì „ ëª¨ë‘ **RoPE Î¸Â·head\_dimÂ·codebook ìˆ˜** ë¥¼ ëœë¤í•´ì‹œ ì„¸íŠ¸ 3-ê°œì”© ë°˜ë³µ í…ŒìŠ¤íŠ¸ â†’ CI ìë™í™” ì‹œ ë§¤íŠ¸ë¦­ìŠ¤í™” ì í•©.
* PT â†’ FL weight copy í•¨ìˆ˜ëŠ” `jax.tree_util.tree_map(lambda p: jnp.array(...))` íŒ¨í„´.

---

## 5. `layer_unit_test_MoshiDecoderLayer.py` 

| ë²”ì£¼           | ì„¸ë¶€ ë‚´ìš©                                                                                                                                                                                 |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **êµ¬ì„±ìš”ì†Œ**     | `MoshiDecoderLayerPT` â‡” `MoshiDecoderLayerFL` (í›„ìëŠ” íŒŒì¼ í•˜ë‹¨ì— ìœ„ì¹˜)<br>ë‚´ë¶€ ëª¨ë“ˆ : Attention, Gating MLP, RMSNorm ëª¨ë‘ **ì „ íŒŒì¼ì—ì„œ import**                                                          |
| **í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤** | â€£ `use_flexible_linear` bool<br>â€£ RoPE on/off<br>â€£ codebook ì¸ë±ìŠ¤ (None / int / 1-D) ì„¸íŒ…ë³„ forward ê²€ì¦                                                                                     |
| **ë²„ê·¸ ê°€ëŠ¥ì„±**   | PT forward ì„œëª…ì€ `hidden_states, attention_mask=None, position_ids=None â€¦`<br>FL ìª½ `__call__` ì€ ê°™ì€ ìˆœì„œì§€ë§Œ **í‚¤ì›Œë“œ ì¸ì ê°•ì œ** â†’ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ì—ì„œ ì´ë¦„ ìˆœì„œ ì‹¤ìˆ˜ ì‹œ Silent fail(**No error, wrong mapping**) ìœ„í—˜ |

---

## 6. `layer_unit_test_DepthDecoder.py` 

| ë²”ì£¼                        | ì„¸ë¶€ ë‚´ìš©                                                                                                                                  |
| ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| **Depth = N-layer ë°˜ë³µ êµ¬ì¡°** | `setup()` ë£¨í‹´ì—ì„œ `setattr(self, f"layer_{i}", MoshiDecoderLayerFL(...))` ë™ì  ë“±ë¡                                                           |
| **I/O ê²½ë¡œ**                | (1) text í† í° ì„ë² ë”© â†’ (B,1,H) <br>(2) audio í† í° ì—¬ëŸ¬ ê°œ â†’ 0-íŒ¨ë”© + ì„ íƒ ì„ë² ë”© (ì½”ë“œ ì°¸ì¡°) <br>(3) `input_linear` ë¡œ `last_hidden_state` íˆ¬ì˜ í›„ residual add |
| **loss**                  | cross-entropy w/ mask (`labels == -100`)                                                                                               |
| **Param Copy**            | `load_depth_decoder_pt_to_flax()` : PT module â†’ Flax params, `jax.random.PRNGKey` ë¡œ ë¹ˆ íŒŒë¼ë¯¸í„° íŠ¸ë¦¬ ìƒì„± í›„ leaf ë‹¨ìœ„ replace                     |
| **ë¦¬ë·° í¬ì¸íŠ¸**                | `embed_tokens` ì˜ ì²« codebook ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©(0) â€” > ë‹¤ì¤‘ ì½”ë“œë¶ í•™ìŠµ ì‹œ í™•ì¥ í•„ìš”                                                                           |


---



ì•„ë˜ ë¹„êµëŠ” **(1) MOSHI\_forTPU ë ˆí¬ì˜ 3 ê°œ ìŠ¤í¬ë¦½íŠ¸**
`layer_unit_test_MoshiModel.py`, `train_model.py`, `train_exp.py`
ì™€ **(2) Kyutai ê³µì‹ Moshi ë ˆí¬**(`kyutai-labs/moshi`)ì˜ ë™ë“±í•œ ë¶€ë¶„(= ì „ì²´ ëª¨ë¸Â·í›ˆë ¨ ì²´ì¸)ì„ **ê¸°ëŠ¥Â·APIÂ·ë°ì´í„° íë¦„** ë‹¨ìœ„ë¡œ ëŒ€ì¡°í•œ ê²ƒì…ë‹ˆë‹¤.

---

## 1  `layer_unit_test_MoshiModel.py` â€” Stub vs ì™„ì œí’ˆ

| í•­ëª©         | MOSHI\_forTPU (íŒŒì¼ í˜„í™©)                                                                                                      | Upstream Moshi (ë™ë“± ê¸°ëŠ¥)                                                                                                      | ì°¨ì´ & í•´ì•¼ í•  ì¼                                                                                                 |
| ---------- | -------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| **ì¡´ì¬ ëª©ì **  | â€œPyTorch â†” Flax *ì „ì²´ ëª¨ë¸* ë™ì¹˜ ê²€ì¦â€ì´ë¼ëŠ” *ìŠ¤í… íŒŒì¼*. ì‹¤ì œ ì½”ë“œëŠ”<br>`python<br># TODO Case A/B/C â€¦<br>pass<br>` ì •ë„ë§Œ ìˆê³  ì‹¤í–‰ ì‹œ ì•„ë¬´ í…ŒìŠ¤íŠ¸ë„ ëŒì§€ ì•ŠìŒ | Upstreamì—ëŠ” ì´ ìˆ˜ì¤€ì˜ *test stub* ì´ ì—†ë‹¤. ëŒ€ì‹  **ì™„ì „í•œ 7 B íŒŒë¼ë¯¸í„° ëª¨ë¸**(PyTorch) ìì²´ë¥¼ ì œê³µí•˜ë©°, í†µí•© í…ŒìŠ¤íŠ¸ëŠ” `pytest` ëª¨ë“ˆë¡œ ëª¨ë¸ forward + loss ì²´í¬ë¥¼ ëŒë¦°ë‹¤ | â‘  **Flaxìš© Moshi 7 B**(Temporal + Depth) í´ë˜ìŠ¤ë¥¼ ë¨¼ì € ì™„ì„±<br>â‘¡ PT â†’ Flax íŒŒë¼ë¯¸í„° ë³µì‚¬ í•¨ìˆ˜ ì‘ì„±<br>â‘¢ ì´ íŒŒì¼ì„ `pytest` ìŠ¤ìœ„íŠ¸ë¡œ êµì²´ |
| **Config** | `DummyConfig` í´ë˜ìŠ¤ë§Œ ìŠ¤ì¼€ì¹˜(4-5 ì†ì„±)                                                                                             | `MoshiConfig` (`hidden_size`, `n_heads`, `codebooks`, â€¦)                                                                    | Kyutai ìŠ¤í‚¤ë§ˆì— ë§ì¶˜ **Flax Config dataclass** ì‹ ì„¤ í•„ìš”                                                              |
| **ê²€ì¦ ë²”ìœ„**  | ì—†ìŒ â†’ ëª¨ë“  assert íŒ¨ìŠ¤                                                                                                          | GPT-style backbone + DepthDecoder + Dual Audio/Text Heads ëª¨ë‘ ì»¤ë²„                                                             | â€œDepth <-> Temporal ì¸í„°í˜ì´ìŠ¤â€ shape, RoPE Î¸, codebook-routing diff â‰¤ 1e-6 ê²€ì¦ ì¶”ê°€                                |
| **ê²°ë¡ **     | **ì»¨ì…‰ ë¬¸ì„œ ìˆ˜ì¤€**. ì‚¬ì‹¤ìƒ â€œì•„ì§ ì•„ë¬´ê²ƒë„ êµ¬í˜„ ì•ˆ ë¨â€ â€”> ë³¸ê²© í…ŒìŠ¤íŠ¸ë¥¼ ì“°ë ¤ë©´ *ì‹¤ì œ ëª¨ë¸ ì •ì˜ + ë¡œë” + íŒŒë¼ë¯¸í„° íŠ¸ë¦¬* ë¶€í„° ì‘ì„±í•´ì•¼ í•¨                                      |                                                                                                                             |                                                                                                             |

---

## 2  í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ë¹„êµ (Flax vs PyTorch)

### 2-A  `train_model.py` (Flax, TPU ìš© ë‹¨ì¼ ì½”ì–´)

| í•­ëª©               | MOSHI\_forTPU êµ¬í˜„                                         | Upstream ëŒ€ì‘ (`scripts/train.py`)            | Gap / êµ¬í˜„ í•„ìš”                                      |
| ---------------- | -------------------------------------------------------- | ------------------------------------------- | ------------------------------------------------ |
| **í”„ë ˆì„ì›Œí¬ & ë””ë°”ì´ìŠ¤** | JAX/Flax, TPU *1 core only*.<br>`--pmap` ì£¼ì„ë§Œ ìˆê³  ì‹¤ì œ í˜¸ì¶œ ì—†ìŒ | PyTorch + DeepSpeed ZeRO-3, GPU ë‹¤ì¤‘ ë…¸ë“œ       | TPU ë©€í‹°ì½”ì–´: `pjit` + `mesh` ìƒ¤ë”©, XLA bf16 ì¶”ë¡         |
| **ë°ì´í„° ë¡œë”**       | `get_dummy_batch()` â€” ë‚œìˆ˜ í† í° ìƒì„±                           | TFRecord / webdataset â‡¢ **Mimi codec í† í°**   | Mimi codec wheel ë¹Œë“œ + `tf.data` ë¡œë” í†µí•©            |
| **ì†ì‹¤ í•¨ìˆ˜**        | ë‹¨ì¼ `cross_entropy` (ì˜¤ë””ì˜¤ í† í°)                              | **ì´ì¤‘ ì†ì‹¤**: â‘  ìŒì„± í† í°, â‘¡ í…ìŠ¤íŠ¸ í† í°(ë‚´ì  ëª¨ë†€ë¡œê·¸), Î» ì¡°í•© | Text prefix lossÂ·Î» ìŠ¤ì¼€ì¤„ ì¶”ê°€                        |
| **ì˜µí‹°ë§ˆì´ì €**        | Optax AdamW (ê³ ì • LR)                                      | AdamW + `cosine_with_warmup` ìŠ¤ì¼€ì¤„            | `optax.warmup_cosine_decay_schedule`, grad accum |
| **ì²´í¬í¬ì¸íŠ¸**        | Orbax (local)                                            | DeepSpeed sharded + WandB resume            | Orbax â†” HF `save_pretrained` ë¸Œë¦¬ì§€                 |
| **ê²°ë¡ **           | â€œFlax ì²˜ìŒ ì†ëŒ€ë³¸ PoC ë£¨í”„â€                                     | â€œì‹¤ì „ 7 B ë‹¤ì¤‘ GPU í•™ìŠµ ë£¨í”„â€                       | LR ìŠ¤ì¼€ì¤„Â·mixed bf16Â·ë¶„ì‚° ShardSpec í•„ìˆ˜                |

### 2-B  `train_exp.py` (PyTorch + ğŸ¤— Accelerate)

| í•­ëª©                  | MOSHI\_forTPU                                             | Upstream                                         | Gap                             |
| ------------------- | --------------------------------------------------------- | ------------------------------------------------ | ------------------------------- |
| **ëŸ°ì²˜**              | `accelerate launch`, DDP 1-8 GPU                          | DeepSpeed CLI(`--deepspeed_config`)              | ZeRO-stage, CPU offload ë¯¸ì§€ì›     |
| **ëª¨ë¸ ë¡œë“œ**           | `MoshiDepthDecoderFL` ë§Œ                                   | `MoshiForConditionalGeneration` 7 B              | Temporal backboneÂ·Dual head ì „ë¬´  |
| **Mixed precision** | Amp bf16 off by default                                   | `torch.bfloat16` native, Flash-Attention kernels | Flash-Attention2, AMP ìŠ¤ì¼€ì¤„ë§      |
| **ê²°ë¡ **              | â€œGPU ìš© ê°€ë²¼ìš´ ì˜ˆì œâ€ ìˆ˜ì¤€                                         | ëŒ€ê·œëª¨ ë””ë°”ì´ìŠ¤Â·ZeRO ìµœì í™”                                | Mimi ë°ì´í„° + 7 B ëª¨ë¸ ë§ì¶° ì „ë©´ ë¦¬ë² ì´ìŠ¤ í•„ìš” |

---

| íŒŒì¼                        | í•µì‹¬ ë‚´ìš©                                        | ìƒíƒœ                                  |
| ------------------------- | -------------------------------------------- | ----------------------------------- |
| **`Readme.md`**           | ë ˆí¬ ëª©ì Â·ëŒ€ìƒ ë ˆì´ì–´ 5ì¢…Â·í…ŒìŠ¤íŠ¸ ì „ëµÂ·TODO ìš”ì•½               | ì´ˆì•ˆ ìˆ˜ì¤€ (ì„¸ë¶€ APIâ€†doc ì—†ìŒ) ([GitHub][1]) |
| **`Note_att_rope.md`**    | RoPE â€œ4 vs 2 broadcastâ€ ì˜¤ë¥˜ ì›ì¸Â·ìˆ˜ì •ë²•Â·ìˆ˜í•™ì  ì •ë‹¹ì„± ì •ë¦¬ | ë‚´ìš© ì¶©ì‹¤, ì½”ë“œ ìŠ¤ë‹ˆí«ê¹Œì§€ í¬í•¨ ([GitHub][2])    |
| **`train_model_todo.md`** | í•™ìŠµ íŒŒì´í”„ë¼ì¸ í™•ì¥ ì²´í¬ë¦¬ìŠ¤íŠ¸                            | ê°„ë‹¨ ì²´í¬ë¦¬ìŠ¤íŠ¸ë§Œ ì¡´ì¬                        |

[1]: https://github.com/Ch-talk-SKM/MOSHI_forTPU "GitHub - Ch-talk-SKM/MOSHI_forTPU"
[2]: https://github.com/Ch-talk-SKM/MOSHI_forTPU/raw/main/Note_att_rope.md?plain=1 "github.com"


